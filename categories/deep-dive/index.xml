<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep dive on Dom Goodwin</title>
    <link>/categories/deep-dive/</link>
    <description>Recent content in Deep dive on Dom Goodwin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>dom@domgoodwin.dev (Dom Goodwin)</managingEditor>
    <webMaster>dom@domgoodwin.dev (Dom Goodwin)</webMaster>
    <lastBuildDate>Thu, 21 Apr 2022 10:06:09 +0000</lastBuildDate><atom:link href="/categories/deep-dive/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kubernetes Networking #2: Services</title>
      <link>/post/kubernetes-networking-2-services/</link>
      <pubDate>Thu, 21 Apr 2022 10:06:09 +0000</pubDate>
      <author>dom@domgoodwin.dev (Dom Goodwin)</author>
      <guid>/post/kubernetes-networking-2-services/</guid>
      <description>&lt;p&gt;Kubernetes services are an essential resource everyone uses, but what are all the different types and how do they work under the hood?&lt;/p&gt;</description>
      <content>&lt;p&gt;Kubernetes services are an essential resource everyone uses, but what are all the different types and how do they work under the hood?&lt;/p&gt;
&lt;p&gt;In Kubernetes pods are ephemeral (or &lt;a href=&#34;https://thenewstack.io/how-to-treat-your-kubernetes-clusters-like-cattle-not-pets/&#34;&gt;cattle&lt;/a&gt;), you should expect them to be killed, scaled and replaced whenever. So how do you talk to a set of pods which might have changed the next second? A service.&lt;/p&gt;
&lt;p&gt;Services sit in front of your application and mean you only have to call the service address and magic Kubernetes components will get that request to the right Pod(s).
How does a service know which pods it can talk to?&lt;/p&gt;
&lt;h2 id=&#34;selectors&#34;&gt;Selectors&lt;/h2&gt;
&lt;p&gt;Services map to pods via label selectors. You have a pod with label &lt;code&gt;app=etizer&lt;/code&gt; and then create a service with &lt;code&gt;.spec.selector&lt;/code&gt; for the same and that service will forward traffic to any Ready pods matching that label.&lt;/p&gt;
&lt;p&gt;Behind the scenes, a service resource also creates an &lt;code&gt;Endpoints&lt;/code&gt; resource which maintains a list of Pod IPs based on your service selector. It’s worth noting here, the readyiness checks you implement for your Pods are what controls whether the IP goes into this &lt;code&gt;Endpoints&lt;/code&gt; resource and allow traffic.&lt;/p&gt;
&lt;p&gt;The exception here is selector-less services, these don’t have a label selector so won’t also have an &lt;code&gt;Endpoint&lt;/code&gt; resource. To use these you also need to create an endpoint resource of the same name as your service, you can add any IPs and ports you want to this resource. This is a great way to map a service to something not in your cluster so your pods can talk to it as if it was a service.&lt;/p&gt;
&lt;p&gt;But what are the different types of services and how do they work?&lt;/p&gt;
&lt;h2 id=&#34;service-types&#34;&gt;Service Types&lt;/h2&gt;
&lt;h3 id=&#34;clusterip&#34;&gt;ClusterIP&lt;/h3&gt;
&lt;p&gt;When you create a &lt;code&gt;ClusterIP&lt;/code&gt; type service you’ll will get:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A virtual IP for the service
&lt;ul&gt;
&lt;li&gt;This is within the &lt;code&gt;--service-cluster-ip-range&lt;/code&gt; your &lt;code&gt;api-server&lt;/code&gt; has been configured with&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube-proxy&lt;/code&gt; is responsible for this virtual IP routing to one of your Pod IPs&lt;/li&gt;
&lt;li&gt;This virtual IP will forward requests to one of your pod IPs based on the &lt;code&gt;Endpoints&lt;/code&gt; resource associated with your service.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A local cluster DNS address, eg. &lt;code&gt;&amp;lt;service-name&amp;gt;.&amp;lt;namespace&amp;gt;.svc.cluster.local&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;This is resolved by your &lt;code&gt;kube-dns&lt;/code&gt; (CoreDNS) to your virtual service IP&lt;/li&gt;
&lt;li&gt;This is why your service name must be &lt;a href=&#34;https://datatracker.ietf.org/doc/html/rfc1035&#34;&gt;RFC 1035 compliant&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The normal way of utilising this would be giving your application the service DNS address so it can talk to another application running on the cluster. This DNS address resolves to the virtual IP and then the configuration &lt;code&gt;kube-proxy&lt;/code&gt; controls handles that IP resolving to actual pod IP addresses. From there your CNI will route your request using the pod IP.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;ClusterIP&lt;/code&gt; service type is ✨&lt;em&gt;special&lt;/em&gt;✨, whilst you use it as a way to expose your service internally within the cluster, it’s also used by types &lt;code&gt;NodePort&lt;/code&gt; and &lt;code&gt;LoadBalancer&lt;/code&gt; (by extension of LoadBalancer using &lt;code&gt;NodePort&lt;/code&gt;) as part of their implementations.&lt;/p&gt;

  &lt;figure class=&#34;left&#34; &gt;
    &lt;a href=&#34;&#34; &gt;
    &lt;img src=&#34;/img/k8s-services-2-types.png&#34;   /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Just like a Matryoshka doll&lt;/figcaption&gt;
    
    &lt;/a&gt;
  &lt;/figure&gt;


&lt;p&gt;&lt;code&gt;&amp;lt;service-ip&amp;gt;:&amp;lt;service-port&amp;gt; → &amp;lt;pod-ip&amp;gt;:&amp;lt;pod-port&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;nodeport&#34;&gt;NodePort&lt;/h3&gt;
&lt;p&gt;A &lt;code&gt;NodePort&lt;/code&gt; type service opens up a port on &lt;strong&gt;every node&lt;/strong&gt; in your cluster, this port then proxies requests to your service’s virtual IP address.&lt;/p&gt;
&lt;p&gt;The port range defaults to &lt;code&gt;30000-32767&lt;/code&gt;, interestingly enough just below the Linux kernel default &lt;a href=&#34;https://www.kernel.org/doc/html/latest//networking/ip-sysctl.html#ip-variables&#34;&gt;ephemeral port range&lt;/a&gt;. It can also be configured with the flag &lt;code&gt;--service-node-port-range&lt;/code&gt; if you need these ports for some use case.&lt;/p&gt;
&lt;p&gt;At a high level a &lt;code&gt;NodePort&lt;/code&gt; is a single rule configured by &lt;code&gt;kube-proxy&lt;/code&gt; onto every node, to forward requests to an underlying &lt;code&gt;ClusterIP&lt;/code&gt; service.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;node-ip&amp;gt;:&amp;lt;node-port&amp;gt; → &amp;lt;service-ip&amp;gt;:&amp;lt;service-port&amp;gt; → &amp;lt;pod-ip&amp;gt;:&amp;lt;pod-port&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;loadbalancer&#34;&gt;LoadBalancer&lt;/h3&gt;
&lt;p&gt;A &lt;code&gt;LoadBalancer&lt;/code&gt; type service only really works in a cloud environment. When created, asyncronously, the &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/cloud-controller/&#34;&gt;cloud-controller-manager&lt;/a&gt; will provision you a load balancer which directs traffic to your pods.&lt;/p&gt;
&lt;p&gt;The implementation of the different cloud load balancers varys wildly. It used to be different clouds were implemented &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/release-1.11/pkg/cloudprovider/providers/aws&#34;&gt;in-tree&lt;/a&gt;, committing directly to kubernetes but now it must be done via out-of-tree implementations, eg. &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-aws&#34;&gt;cloud-provider-aws&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Cloud implementations aside, once you have a &lt;em&gt;cloud&lt;/em&gt; load balancer it works by pointing to a &lt;code&gt;NodePort&lt;/code&gt; service running. This means that you have a load balancer with every Kubernetes worker node as a listener, forwarding traffic to any nodes on a particular port. From there it follows the standard &lt;code&gt;NodePort&lt;/code&gt; → &lt;code&gt;ClusterIP&lt;/code&gt; route.&lt;/p&gt;
&lt;p&gt;If you didn’t want to write a whole &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/&#34;&gt;cloud control manager&lt;/a&gt; to expose your application on your own servers, a &lt;code&gt;NodePort&lt;/code&gt; service with that port pointed to from your existing load balancer would do the trick.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;load-balancer-ip&amp;gt;:&amp;lt;lb-port&amp;gt; -&amp;gt; &amp;lt;node-ip&amp;gt;:&amp;lt;node-port&amp;gt; → &amp;lt;service-ip&amp;gt;:service-port&amp;gt; → &amp;lt;pod-ip&amp;gt;:&amp;lt;pod-port&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;externalname&#34;&gt;ExternalName&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ExternalName&lt;/code&gt; services are the odd one out. Instead of using good ol’ &lt;code&gt;ClusterIP&lt;/code&gt; and getting a virtual IP, they create a &lt;code&gt;CNAME&lt;/code&gt; record to map your service DNS to another DNS address.&lt;/p&gt;
&lt;p&gt;As an example use case, lets say you have different environment databases, instead of service-owners changing the DNS depending on where they deploy their pods to you create a &lt;code&gt;ExternalName&lt;/code&gt; service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;apiVersion: v1
kind: Service
metadata:
  name: database
  namespace: prod
spec:
  type: ExternalName
  externalName: prod.database.example.com
---
apiVersion: v1
kind: Service
metadata:
  name: database
  namespace: dev
spec:
  type: ExternalName
  externalName: dev.database.example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now all the service-owners have to do is point their code to &lt;code&gt;database.svc.cluster.local&lt;/code&gt; and they will talk to the right database every time.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;database.svc.cluster.local&lt;/code&gt; = &lt;code&gt;CNAME test.database.example.com&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;It’s worth noting since the pod requesting the service will get the &lt;code&gt;CNAME&lt;/code&gt; record back, the pod also has to be configured to resolve whatever domain the &lt;code&gt;CNAME&lt;/code&gt; returns. If it’s a private record, you might want to &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/&#34;&gt;configure revolvers in CoreDNS&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;how-virtual-ips-work&#34;&gt;How Virtual IPs work&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Most&lt;/em&gt; services (sorry, &lt;code&gt;ExternalName&lt;/code&gt;) use a virtual IP to forward requests to the proper pods. It’s the responsibility of &lt;code&gt;kube-proxy&lt;/code&gt; to provide this virtual IP and make sure the traffic is actually forwarded. &lt;code&gt;kube-proxy&lt;/code&gt; runs on every node so the steps below happen per worker node.&lt;/p&gt;
&lt;p&gt;How this works depends on the mode of &lt;code&gt;kube-proxy&lt;/code&gt; is given with &lt;code&gt;-proxy-mode&lt;/code&gt; flag:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ℹ️ &lt;strong&gt;For all of these &lt;code&gt;kube-proxy&lt;/code&gt; will monitor the control plane for new/updated/removed &lt;code&gt;Service&lt;/code&gt; and &lt;code&gt;Endpoint&lt;/code&gt; resources and run the following steps&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;userspace&lt;/strong&gt; (legacy)
&lt;ul&gt;
&lt;li&gt;For each &lt;code&gt;Service&lt;/code&gt; it opens a random proxy port on the node&lt;/li&gt;
&lt;li&gt;This proxy port is then told to iptables to capture traffic to this particular virtual IP and port to redirect to the Pod IPs in the &lt;code&gt;Endpoint&lt;/code&gt; resource&lt;/li&gt;
&lt;li&gt;The backend pod is chosen based on the &lt;code&gt;SessionAffinity&lt;/code&gt; of the service&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The traffic here is actually proxied via &lt;code&gt;kube-proxy&lt;/code&gt; itself&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;iptables&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;For each &lt;code&gt;Service&lt;/code&gt; it creates iptables rules which redirect traffic to the virtual IP and service port&lt;/li&gt;
&lt;li&gt;These redirect rules point to sets of pod IPs based on what is in the endpoint resource&lt;/li&gt;
&lt;li&gt;The backend pod is chosen at random by default&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The traffic is all handled within the Linux &lt;a href=&#34;https://www.netfilter.org/&#34;&gt;netfilter&lt;/a&gt; so all within the kernelspace, &lt;code&gt;kube-proxy&lt;/code&gt; doesn’t handle the traffic it just sets up rules to process it&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IPVS&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Whilst iptables was designed to be a firewall and &lt;code&gt;kube-proxy&lt;/code&gt; just uses it to redirect network traffic, IPVS was made for load balancing&lt;/li&gt;
&lt;li&gt;At a high level this works similar to iptables, cluster IP and port → set of pod IPs&lt;/li&gt;
&lt;li&gt;IPVS is important when working at scale, if you have &amp;gt;1000 services (or 10,000 pods) in your cluster IPVS is the performant choice&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;kernelspace&lt;/strong&gt; (windows)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;bringing-it-all-together&#34;&gt;Bringing it all together&lt;/h2&gt;
&lt;p&gt;Let’s create an example &lt;code&gt;LoadBalancer&lt;/code&gt; service and see what components come together to get traffic from a client to your pods. We’ll show how traffic from outside, via the load balancer, gets to your pods and how another pod in the cluster can also use this service without going via the load balancer.&lt;/p&gt;
&lt;p&gt;In this example let’s say we’re running on a Kubernetes cluster in AWS with the AWS cloud controller manager properly configured, we have pods running with label &lt;code&gt;app=alpha&lt;/code&gt; which listens on port &lt;code&gt;8080&lt;/code&gt; for traffic and &lt;code&gt;kube-proxy&lt;/code&gt; is in the default &lt;code&gt;iptables&lt;/code&gt; mode.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Service YAML&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;apiVersion: v1
kind: Service
metadata:
&lt;span style=&#34;color:#75715e&#34;&gt;# This doesn&amp;#39;t affect routing but it&amp;#39;s best practice to label components together&lt;/span&gt;
  labels:
    app: alpha
  name: example
spec:
  ports:
  - name: main
    port: &lt;span style=&#34;color:#ae81ff&#34;&gt;8081&lt;/span&gt;
    protocol: TCP
&lt;span style=&#34;color:#75715e&#34;&gt;# Target port is the port your pod has open&lt;/span&gt;
    targetPort: &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# This is what tells the service which pods to route to&lt;/span&gt;
  selector:
    app: alpha
  type: LoadBalancer
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lets create the service and have a look:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl apply -f service.yaml
service/example created

$ kubectl get service example
NAME      TYPE           CLUSTER-IP      EXTERNAL-IP     PORT&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;          AGE
example   LoadBalancer   10.43.205.153   192.168.0.147   8081:32255/TCP   4s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I now have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An Endpoints resource corresponding to ready pods matching my label selector
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubectl get endpoints example&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A cluster IP setup for my service &lt;code&gt;10.43.205.153&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A DNS address for the service configured
&lt;ul&gt;
&lt;li&gt;(from inside a Pod)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dig example.default.svc.cluster.local&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A external (to the cluster) IP which maps to my load balancer &lt;code&gt;192.168.0.147&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A NodePort setup &lt;code&gt;32255&lt;/code&gt; so all my workers now have this port open
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;netstat -tulnp | grep 32255&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iptables rules setup
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sudo iptables -L | grep example&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo iptables -L | grep 10.43.205.153&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lets see how this request goes through to our pod from outside the cluster and a pod within.&lt;/p&gt;
&lt;h3 id=&#34;from-outside-the-cluster&#34;&gt;From outside the cluster&lt;/h3&gt;

  &lt;figure class=&#34;left&#34; &gt;
    &lt;a href=&#34;&#34; &gt;
    &lt;img src=&#34;/img/k8s-services-2-lb-external.png&#34;   /&gt;
    
    &lt;/a&gt;
  &lt;/figure&gt;


&lt;ol&gt;
&lt;li&gt;The client makes a request to the load balancer to try and talk to the application, for how this is configured see my post on ingress into a cluster&lt;/li&gt;
&lt;li&gt;The load balancer selects one of the configured listeners, this is usually either random or round robin but depends on your load balancer and configuration&lt;/li&gt;
&lt;li&gt;The node has port 32255 open, requests here are configured in iptables to forward the request to the virtual IP of the service. These rules were configured by &lt;code&gt;kube-proxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;iptables then resolves the virtual/service IP into a Pod IP address, one is chosen by random and the request heads there. Now the traffic is inside the cluster the CNI controls the network movements between nodes (see &lt;a href=&#34;https://dgood.win/post/kubernetes-networking-overview/&#34;&gt;here&lt;/a&gt; for more info on what the CNI sets up)&lt;/li&gt;
&lt;li&gt;Once the request has reached the node with the pod on it, the CNI and iptables will forward it to the Pod as expected&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;from-a-pod-within-the-cluster&#34;&gt;From a pod within the cluster&lt;/h3&gt;

  &lt;figure class=&#34;left&#34; &gt;
    &lt;a href=&#34;&#34; &gt;
    &lt;img src=&#34;/img/k8s-services-2-lb-internal.png&#34;   /&gt;
    
    &lt;/a&gt;
  &lt;/figure&gt;


&lt;ol&gt;
&lt;li&gt;The requesting pod will have been configured with the service DNS address &lt;code&gt;example.default.svc.cluster.local&lt;/code&gt;. As it makes this request from within the cluster CoreDNS running will be the DNS resolver. This request will be resolved to the Service (virtual) IP address: &lt;code&gt;10.43.205.153&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Making a request to this address will hit the rules in iptables, configured by &lt;code&gt;kube-proxy&lt;/code&gt;, this resolves the service IP into a Pod IP by randomly selecting a pod&lt;/li&gt;
&lt;li&gt;The rest of the request continues as in the external example.&lt;/li&gt;
&lt;/ol&gt;</content>
    </item>
    
    <item>
      <title>Kubernetes Networking #1</title>
      <link>/post/kubernetes-networking-overview/</link>
      <pubDate>Mon, 10 Jan 2022 12:06:09 +0000</pubDate>
      <author>dom@domgoodwin.dev (Dom Goodwin)</author>
      <guid>/post/kubernetes-networking-overview/</guid>
      <description>&lt;p&gt;Kubernetes networking can be a bit of a rabbit hole to debug and discover what your pods are actually doing. In this post we’ll explore at a high level how a request goes from 1 pod to another and which components control each.&lt;/p&gt;</description>
      <content>&lt;p&gt;Kubernetes networking can be a bit of a rabbit hole to debug and discover what your pods are actually doing. In this post we’ll explore at a high level how a request goes from 1 pod to another and which components control each.&lt;/p&gt;
&lt;h2 id=&#34;components&#34;&gt;Components&lt;/h2&gt;
&lt;p&gt;The components we’re going to look at today are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kube-dns/coredns&lt;/code&gt; resolves domain names for services (and pods) into an IP address
&lt;ul&gt;
&lt;li&gt;ℹ️ &lt;code&gt;kube-dns&lt;/code&gt; was replaced by &lt;code&gt;coredns&lt;/code&gt; but the in cluster service is still called &lt;code&gt;kube-dns&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;This is the resolver for any DNS requests inside your pods, if you wanted to resolve internal service domains or to a private DNS resolver, you could configure this to forward those requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube-proxy&lt;/code&gt; resolves network requests for the IP address of a service to the IP addresses of pods which match that service. This has different modes:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Iptables&#34;&gt;iptables&lt;/a&gt; (default) - To see your iptables rules: &lt;code&gt;iptables -L&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/&#34;&gt;ipvs&lt;/a&gt; - To see your ipvs rules &lt;code&gt;ipvsadm -Ln&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Your chosen &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/networking/#calico&#34;&gt;CNI&lt;/a&gt;, there’s a range of these, all which work differently. Generally a CNI will configure any networking around a Pod IP, routing to and from pods to:
&lt;ul&gt;
&lt;li&gt;Ensure pod network requests are routed properly to other pods&lt;/li&gt;
&lt;li&gt;Ensure pod network requests can reach outside the cluster&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An easy way of thinking about it can be, resolving between a service and healthy pods is &lt;code&gt;kube-dns&lt;/code&gt; and &lt;code&gt;kube-proxy&lt;/code&gt; but then that request will need to go through routing setup by the &lt;code&gt;CNI&lt;/code&gt;. For cloud environments, your &lt;code&gt;CNI&lt;/code&gt; will usually handle (or provide ways to setup) things like cross-subnet traffic as well.&lt;/p&gt;
&lt;p&gt;To follow the resolution between components, say we wanted to go to a service called &lt;code&gt;myapp&lt;/code&gt; which sits in namespace &lt;code&gt;namespace001&lt;/code&gt; which had 2 healthy pods matching the service selector. The resolution would be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;CoreDNS:&lt;/strong&gt; (service FQDN)&lt;code&gt;myapp.namespace001.svc.cluster.local&lt;/code&gt; → &lt;code&gt;172.16.10.10&lt;/code&gt; (service cluster IP)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;iptables:&lt;/strong&gt; &lt;code&gt;172.16.10.10&lt;/code&gt; → &lt;code&gt;10.10.10.10&lt;/code&gt; (a pod IP, selected at random from healthy pods)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CNI:&lt;/strong&gt; Will route traffic for &lt;code&gt;10.10.10.10&lt;/code&gt; from the requester node, to the target node&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;example-journey-of-a-request&#34;&gt;Example journey of a request&lt;/h2&gt;
&lt;p&gt;As an example, let’s explore in an example Kubernetes cluster how a pod will send a network request to another pod.
For this example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The pod will try to talk to a pod on another node then itself&lt;/li&gt;
&lt;li&gt;The 2 nodes will be in seperate subnets
&lt;ul&gt;
&lt;li&gt;Routing between subnets is presumed to be setup and fine and we’ll abstract that to a &lt;code&gt;router&lt;/code&gt; in the diagram&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;CNI&lt;/code&gt; will be Calico, but we won’t be looking too much into its internals&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pod-0&lt;/strong&gt; will send a request to &lt;strong&gt;pod-3&lt;/strong&gt; through a service&lt;/li&gt;
&lt;li&gt;We’ll assume Kubernetes and Calico are setup with defaults:
&lt;ul&gt;
&lt;li&gt;iptables mode for &lt;code&gt;kube-proxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;vxlan encapsulation for cross subnet traffic&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

  &lt;figure class=&#34;left&#34; &gt;
    &lt;a href=&#34;/img/k8s_network_request_example.png&#34; &gt;
    &lt;img src=&#34;/img/k8s_network_request_example.png&#34;   /&gt;
    
    &lt;/a&gt;
  &lt;/figure&gt;


&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt; &lt;strong&gt;pod-0&lt;/strong&gt; requests the service’s Cluster IP from &lt;code&gt;kube-dns&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kube-dns&lt;/code&gt; has dynamic config which updates mapping the service name to a Service IP address lets say &lt;code&gt;172.16.10.10&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;`kubectl exec command for coredns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;*technically this whole request to &lt;code&gt;kube-dns&lt;/code&gt; goes via the same iptables→routing tables path since &lt;code&gt;kube-dns&lt;/code&gt; runs in pods on the cluster. The only key difference is the DNS resolver is an IP address so it doesn’t ask &lt;code&gt;kube-dns&lt;/code&gt; to define &lt;code&gt;kube-dns&lt;/code&gt;. But trying to show aswell would be madness.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;2&lt;/strong&gt; With the cluster IP (&lt;code&gt;172.16.10.10&lt;/code&gt;) the request hits iptables&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;iptables selects from a random one of the pod IP addresses for this particular service IP
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;iptables -L&lt;/code&gt; or &lt;code&gt;iptables-save&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;These rules are all loaded in here and dynamically updated by `kube-proxy If a pod starts failing Readiness checks for instance, it gets removed from this list.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;3&lt;/strong&gt; The Pod IP is now resolved in the nodes routing tables &lt;code&gt;sudo route -n&lt;/code&gt; or &lt;code&gt;ip route list&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These routing table rules will be configured based on the network environment the node sits in. Calico, running locally, will also configure rules for pod networking.&lt;/li&gt;
&lt;li&gt;Based on the Pod IP, it’ll be resolved through multiple sets of rules:
&lt;ul&gt;
&lt;li&gt;This node will have a range of Pod IPs which resolve to the local node itself&lt;/li&gt;
&lt;li&gt;This node will also have an IP range for nodes inside the same subnet - these could avoid encapsulation for performance gain&lt;/li&gt;
&lt;li&gt;This node also has range(s) for other subnets, if it’s not local to the node or subnet, it then gets routed to the subnet routing table. For these rules, Calico might instead route traffic to itself to be encapsulated before crossing the network boundary.&lt;/li&gt;
&lt;li&gt;There also might be a range for external traffic not local to the network, but if this was a private subnet this is most likely behind a NAT gateway inside the local network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This particular request is destined for another subnet. The routing table has directed the request to the Calico pod running locally on this node to handle routing this.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt; Calico will now encapsulate the network request to allow it to pass subnet boundaries &lt;code&gt;calicoctl node status&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By encapsulating the request’s original networking information is wrapped up as data and new network info is added&lt;/li&gt;
&lt;li&gt;Calico will send this as a vxlan or ipinip packet to the required node in another subnet&lt;/li&gt;
&lt;li&gt;The encapsulated packet (shown as an orange line) will:
&lt;ul&gt;
&lt;li&gt;Go back through the route table on the node itself&lt;/li&gt;
&lt;li&gt;To the local subnet router, which will resolve which subnet it needs to go into&lt;/li&gt;
&lt;li&gt;Hit the destination subnet routing table which can direct it to the node&lt;/li&gt;
&lt;li&gt;To the other node itself&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt; Once the encapsulted packet arrived at the other node, it’ll go through iptables rules to see if it’s allowed traffic.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is why you might need to add the encapsulated packets into your firewall rules to be allowed in.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;6&lt;/strong&gt; The routing table on this node will now:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Send the encapsulated packet to the local Calico to be unencapsulated&lt;/li&gt;
&lt;li&gt;The original (unencapsulated) network request will resolve again in iptables and routing tables rules to determine where it should go
&lt;ul&gt;
&lt;li&gt;This could also cause the packet to have to be routed again since the destination has moved&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The request is directed to the pod running locally&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;there-are-a-lot-of-things-missed-off-here&#34;&gt;There are a lot of things missed off here:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.adminsehow.com/2011/09/iptables-packet-traverse-map/&#34;&gt;Technically iptables is interacted with before, after (and sometimes during) a routing decision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How the pod sits within it’s own &lt;a href=&#34;https://blog.scottlowe.org/2013/09/04/introducing-linux-network-namespaces/&#34;&gt;network namespace&lt;/a&gt; with a virtual ethernet interface for traffic coming to the pod&lt;/li&gt;
&lt;li&gt;How Calico uses &lt;a href=&#34;https://projectcalico.docs.tigera.io/reference/felix/&#34;&gt;Felix&lt;/a&gt; to program it’s routes and ACLs and &lt;a href=&#34;https://bird.network.cz/&#34;&gt;BIRD&lt;/a&gt; for distributing the routes to BGP peers on the network&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/networking/&#34;&gt;Alternate CNIs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How the whole kube-proxy aspects could be stipped out for an eBPF approach (Calico’s &lt;a href=&#34;https://projectcalico.docs.tigera.io/maintenance/ebpf/enabling-bpf&#34;&gt;eBPF dataplane&lt;/a&gt; or Cilium and it’s &lt;a href=&#34;https://docs.cilium.io/en/v1.9/gettingstarted/kubeproxy-free/&#34;&gt;kube proxy replacement&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-docs&#34;&gt;Related docs:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://projectcalico.docs.tigera.io/reference/architecture/overview&#34;&gt;Calico Component Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/&#34;&gt;kube-proxy reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/&#34;&gt;Kubernetes DNS docs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>
    </item>
    
  </channel>
</rss>
