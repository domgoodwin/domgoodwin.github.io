<?xml version="1.0" encoding="utf-8" standalone="yes"?><?xml-stylesheet href="/feed_style.xsl" type="text/xsl"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="https://www.rssboard.org/media-rss">
  <channel>
    <title>Kubernetes on Dom Goodwin</title>
    <link>https://dgood.win/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on Dom Goodwin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Dom Goodwin</copyright>
    <lastBuildDate>Thu, 12 Dec 2024 08:06:09 +0000</lastBuildDate><atom:link href="https://dgood.win/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" /><icon>https://dgood.win/logo.svg</icon>
    
    
    <item>
      <title>Talk: Scaling to Success</title>
      <link>https://dgood.win/posts/talk-scaling-to-success/</link>
      <pubDate>Thu, 12 Dec 2024 08:06:09 +0000</pubDate>
      
      <guid>https://dgood.win/posts/talk-scaling-to-success/</guid>
      <description><![CDATA[<p>I recently gave a talk at Kubernetes Community Days UK on the history
of how Monzo autoscaled it&rsquo;s Kubernetes cluster and more recently how we&rsquo;ve
optimised for cost using Karpenter and Spot.</p>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/f-2Ps_Ney3c?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>]]></description>
      
    </item>
    
    
    
    <item>
      <title>Kubernetes Networking #3: Ingress</title>
      <link>https://dgood.win/posts/kubernetes-networking-ingress/</link>
      <pubDate>Fri, 22 Jul 2022 18:06:09 +0000</pubDate>
      
      <guid>https://dgood.win/posts/kubernetes-networking-ingress/</guid>
      <description><![CDATA[<p>Kubernetes Ingress provide a way to expose HTTP(S) routes into your pods from a centralized controller and load balancer. It means you can define <em>how</em> the network traffic can get to your pod inside a Kubernetes resource and a controller hosted in the cluster takes care of the implementation.</p>
<p>In a <a href="https://en.wikipedia.org/wiki/Utopia">world without Kubernetes</a>, you might have a reverse proxy deployed where you write the configuration file to detail how to expose your applications in the backend. An ingress controller takes that reverse proxy and automatically generates the config based on the <code>Ingress</code> Kubernetes resources you create. You don‚Äôt have to create a new <code>LoadBalancer</code> service and cloud load balancer for each service you want to expose.</p>
<p>In this guide, we‚Äôll use the Kubernetes maintained ingress controller, <a href="https://github.com/kubernetes/ingress-nginx">ingress-nginx</a>, which uses nginx as it‚Äôs reverse proxy. It‚Äôs important to note, this isn‚Äôt the <a href="https://docs.nginx.com/nginx-ingress-controller/">nginx-ingress-controller</a> from NGINX themselves which has hooks into NGINX plus and other paid offerings. The docs for ingress-nginx can be found <a href="https://kubernetes.github.io/ingress-nginx/">here</a>.</p>
<h2 id="-why-not-just-loadbalancer">ü§∑ Why not just LoadBalancer?</h2>
<p>Kubernetes provides a way for a cloud provisioned load balancer (an <a href="https://aws.amazon.com/elasticloadbalancing/">AWS elastic load balancer</a> for instance) to be deployed by the control plane, see my <a href="https://dgood.win/post/kubernetes-networking-2-services/">services post</a> for more info. We can use these to expose a single service (and collection of pods) from a load balancer. But what happens when you have 50 services deployed onto your cluster? Would you have 50 load balancers for them all? That‚Äôs ~$810 in standing charges every month not including paying for the traffic that goes through them.</p>
<p>This is where an ingress controller comes in. An Ingress controller is a centralized place for traffic to enter your cluster via a single load balancer which then is routed to your backend services automatically. This is all based on <code>Ingress</code> resources deployed on the cluster, through the reverse proxy.
It also introduces a place to add some control over your traffic routing. kube-proxy will default (when in <code>iptables</code> mode) to TCP load balancing traffic randomly, the traffic coming in via a load balancer goes to any pods. With the controls you can configure affinities, round-robin routing, rate limits, CORS etc. to really shape the traffic going to your pods centrally.</p>
<p>These diagrams compare the path a request would take with/without an ingress controller exposing your pod:</p>
<figure><img src="/img/ingress-lb-to-pod.png"
    alt="Without: Application Service LoadBalancer ‚Üí NodePort ‚Üí Randomly to one of your pods"><figcaption>
      <h4>Without: Application Service LoadBalancer ‚Üí NodePort ‚Üí Randomly to one of your pods</h4><p>Without: Application Service LoadBalancer ‚Üí NodePort ‚Üí Randomly to one of your pods</p>
    </figcaption>
</figure>

<figure><img src="/img/ingress-controller-to-pod.png"
    alt="With: Ingress-controller service LoadBalancer ‚Üí NodePort ‚Üí Randomly to an ingress-controller pod ‚Üí To one of your application pods via ClusterIP"><figcaption>
      <h4>With: Ingress-controller service LoadBalancer ‚Üí NodePort ‚Üí Randomly to an ingress-controller pod ‚Üí To one of your application pods via ClusterIP</h4><p>With: Ingress-controller service LoadBalancer ‚Üí NodePort ‚Üí Randomly to an ingress-controller pod ‚Üí To one of your application pods via ClusterIP</p>
    </figcaption>
</figure>

<p>There are some tradeoffs to using an ingress controller:</p>
<ul>
<li>This introduces an extra hop for your network requests to reach your pod. If you have high throughput - and low latency is important - this could be a consideration</li>
<li>In the same vein, the ingress controller pods also use resources. If you have the ingress controller doing a lot of TLS termination or handling high load the resource usage will increase significantly</li>
<li>You can only really use this for (mostly) Layer 7 traffic, TCP or UDP cannot directly be exposed this way</li>
</ul>
<h2 id="-so-what-does-an-ingress-controller-actually-do">üõ†Ô∏è So what does an ingress controller actually do?</h2>
<p>At a high level you have the following components:</p>
<ul>
<li><strong>Kubernetes resources:</strong>
<ul>
<li>Service: type <code>LoadBalancer</code>, meaning your ingress-controller pods are fronted by a cloud load balancer and exposed outside the cluster</li>
<li>Deployment: Ingress Controller itself (see components below for a breakdown)</li>
<li>(sometimes)Deployment: default backend, this is where the ingress controller routes traffic to when there isn‚Äôt a matching host/path rule for the traffic coming in</li>
</ul>
</li>
<li><strong>Components of the ingress controller:</strong>
<ul>
<li>Reverse proxy: This is what actually routes the traffic coming in to downstream services/pods. If you installed nginx on a service and ran it it‚Äôs this basically</li>
<li>Config generator: This continuously gets the state of Ingress resources in the Kubernetes cluster (think of it as spamming <code>kubectl get ingress -A</code> every X-seconds) and uses that to generate config that the reverse proxy (nginx in this case) understands</li>
</ul>
</li>
</ul>
<p>The <em>reverse proxy</em> continually has it‚Äôs config updated by the <em>config generator</em> and then traffic coming into the cluster via the <em>Loadbalancer service</em> hits the reverse proxy and is routed to the proper backend service/pods.</p>
<h2 id="-what-about-ssl">üîí What about SSL?</h2>
<p>For HTTPS traffic, we can configure the ingress-controller to terminate the SSL and provide the certs as needed.</p>
<p>When creating an ingress resource you can specify a Kubernetes Secret of type <code>tls</code> and the ingress-controller will use that secret to terminate the SSL of the request at the reverse proxy.</p>
<p>This could also be coupled with a tool like <a href="https://cert-manager.io/docs/">cert-manager</a> to dynamically issue and renew certificates for you.</p>
<h2 id="-load-balancerreverse-proxy-controls">üéõÔ∏è Load balancer/Reverse proxy controls</h2>
<p>As mentioned before, by having your pods traffic route via a reverse proxy in the cluster you gain more control over the routing and connection. Some useful examples for ingress-nginx are:</p>
<ul>
<li><strong>Authentication</strong>: Setup <a href="https://datatracker.ietf.org/doc/html/rfc2617">basic or digest access authentication</a> on routes
<ul>
<li><code>nginx.ingress.kubernetes.io/auth-type: basic</code></li>
</ul>
</li>
<li><strong>Session affinity:</strong> Allows for things like sticky sessions, keeping a user going to the same backend pod or use cookie affinity to apply a <code>SameSite</code> sticky cookie
<ul>
<li><code>nginx.ingress.kubernetes.io/affinity: sticky</code></li>
</ul>
</li>
<li><strong>SSL passthrough:</strong> Instead of terminating TLS at the load balancer you can let the backend handle it directly
<ul>
<li><code>nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;</code></li>
</ul>
</li>
<li><strong>Redirections:</strong> Both <a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#permanent-redirect">permanent</a> and <a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#temporal-redirect">temporary</a>
<ul>
<li><code>nginx.ingress.kubernetes.io/permanent-redirect: https://www.google.com</code></li>
</ul>
</li>
<li><strong>Security controls</strong>
<ul>
<li><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#ssl-ciphers">SSL ciphers</a> can be configured controlling which SSL ciphers you support
<ul>
<li><code>nginx.ingress.kubernetes.io/ssl-ciphers: &quot;ALL:!aNULL:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP&quot;</code></li>
</ul>
</li>
<li><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#enable-cors">CORS rules</a> to configure the cross-original resource sharing rules
<ul>
<li><code>nginx.ingress.kubernetes.io/enable-cors: &quot;true&quot;</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>A lot of these controls can be configured either as <a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">annotations</a> directly on Ingress resources affecting only those specific routes, or can be provided in the <a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/">Configmap</a> to be applied at a global level.</p>
<h2 id="-can-i-have-multiple-ingress-controllers">ü§π‚Äç‚ôÇÔ∏è Can I have multiple ingress-controllers?</h2>
<p><strong>Yes</strong>. Let‚Äôs say you want to be able to accept traffic from the public internet as well as internally from inside your private network.</p>
<p>We would:</p>
<ul>
<li>Deploy two sets of ingress-controllers, <em>ingress-public</em> and <em>ingress-private</em></li>
<li>Configure them to be public and private. This is done usually by the annotations given to the different <code>Service</code> resources (<code>service.beta.kubernetes.io/aws-load-balancer-scheme: &quot;internet-facing&quot;</code> for instance)</li>
<li>Setup the Ingress classes
<ul>
<li>This is a relatively new resource, it used to be managed by an annotation <code>kubernetes.io/ingress.class</code> which is being deprecated</li>
</ul>
</li>
<li>When creating <code>Ingress</code> resources you can target the <code>IngressClass</code> to specify which controller should setup routing for your traffic</li>
</ul>
<h2 id="-what-about-hostnames">üìÆ What about hostnames?</h2>
<p>Ingress resources in Kubernetes allow you to specify both the Paths and the Host values to route traffic for.</p>
<p>To have your ingress controller exposed via a friendly DNS address you‚Äôd need to point a record of one to your load balancers DNS address. In AWS, this would be creating a Route53 A record aliased to your ELBs address. This would mean anything going to <code>friendly-address.domain.com</code> would point to your cloud-provisioned ELB and then onto your reverse proxy pods from there.</p>
<p>Then any routes off this hostname would be directed to your specified pods, ie.:</p>
<pre tabindex="0"><code>`friendly-address.domain.com/app-a` ‚Üí `svc/app-a` in the cluster
`friendly-address.domain.com/app-b` ‚Üí `svc/app-b` in the cluster and so on
</code></pre><p>But what if you wanted to have multiple different hostnames, maybe 1 per application.</p>
<pre tabindex="0"><code>`app-a.friendly-address.domain.com/` ‚Üí `svc/app-a`
`app-b.friendly-address.domain.com/` ‚Üí `svc/app-b`
</code></pre><p>These would mean adding a new Route53 entry manually every time you want to deploy a new application. It takes away from the benefits of defining Ingress as a Kubernetes resource if you need to go click-ops or Terraform a new host each time.</p>
<p>That‚Äôs where a tool like <a href="https://github.com/kubernetes-sigs/external-dns">external-dns</a> comes in.</p>
<h3 id="external-dns">external-dns</h3>
<p>external-dns allows you to automatically configure your DNS services based on your Kubernetes Ingress resources (and services!).</p>
<p>As an example, if you have deployed external-dns and wanted to setup a new <code>app-c.friendly-address.domain.com</code> to point to your service all you would need to do is create an <code>Ingress</code> resource with the <code>Host</code> field set to <code>app-c.friendly-address.domain.com</code> and external-dns would provision you an A record pointing to your load balancer.</p>
<h2 id="-bringing-it-all-together">üçµ Bringing it all together</h2>
<p>To give an example of all of this in use, lets build resources for the following requirements:</p>
<ul>
<li>We have 3 applications: <code>alpha</code> <code>beta</code> and <code>gamma</code> running on our cluster</li>
<li>We want <code>alpha</code> and <code>beta</code> exposed via the same Hostname but with separate routes:
<ul>
<li><code>public.example.com/alpha</code> and <code>public.example.com/beta</code></li>
</ul>
</li>
<li>We want <code>gamma</code> to have it‚Äôs own hostname as it‚Äôs a separate, unrelated application
<ul>
<li><code>gamma.public.example.com</code></li>
</ul>
</li>
<li>We have the TLS key and cert for <code>public.example.com</code> and want the ingress-controller to terminate TLS for this host</li>
<li><code>gamma</code> handles it‚Äôs own TLS so we shouldn‚Äôt terminate it at the reverse proxy,</li>
</ul>
<p>We already have:</p>
<ul>
<li>A Kubernetes cluster running in AWS</li>
<li>Deployed external-dns onto our cluster configured for AWS Route 53, [<a href="https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md">link</a>]</li>
<li>Deployed ingress-nginx onto our cluster configured for AWS, [<a href="https://kubernetes.github.io/ingress-nginx/deploy/">link</a>]. Ingress class is set to <code>default</code></li>
</ul>
<p>Feel free to try these steps above for yourself the implementation and breakdown of what we‚Äôre doing come below though so spoiler alert.</p>
<hr>
<ol>
<li>
<p>Create the TLS secret for our alpha/beta endpoint</p>
<pre tabindex="0"><code>kubectl create secret tls public \
  --cert=tls.cert \
  --key=tls.key
</code></pre></li>
<li>
<p>YAML for the Ingress of <code>alpha</code> and <code>beta</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">networking.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">alpha-beta</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ingressClassName</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tls</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">public.example.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">public</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">public.example.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">http</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/alpha</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">backend</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">service</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">name</span>: <span style="color:#ae81ff">alpha</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">port</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">number</span>: <span style="color:#ae81ff">8080</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/beta</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">backend</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">service</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">name</span>: <span style="color:#ae81ff">beta</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">port</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">number</span>: <span style="color:#ae81ff">8080</span>
</span></span></code></pre></div></li>
<li>
<p>YAML for the Ingress of <code>gamma</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">networking.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gamma</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">nginx.ingress.kubernetes.io/ssl-passthrough</span>: <span style="color:#e6db74">&#34;true&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ingressClassName</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">gamma.public.example.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">http</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">backend</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">service</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gamma</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">port</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">number</span>: <span style="color:#ae81ff">8080</span>
</span></span></code></pre></div></li>
</ol>
<blockquote>
<p>‚òù SSL Passthrough also needs to be enabled on the ingress-nginx deployment with the flag <code>--enable-ssl-passthrough</code></p>
</blockquote>
<p><strong>What happens with these resources?</strong></p>
<ul>
<li><strong>ingress-nginx:</strong>
<ul>
<li>Constantly checks for new/updated <code>Ingress</code> resources</li>
<li>Detects <code>alpha-beta</code>:
<ul>
<li>Adds nginx config to route to the services
<ul>
<li>Can be checked with: <code>kubectl exec -it &lt;ingress-controller-pod-name&gt; -- cat /etc/nginx/nginx.conf</code></li>
<li>This config includes the secret mentioned to be able to terminate TLS</li>
</ul>
</li>
<li>Updates the <code>Ingress</code> resource <code>Address</code> field with the DNS of the Cloud load balancer the controller knows is pointing at it</li>
<li><code>curl https://public.example.com/alpha -v</code> shows it routing correctly and the cert as expected</li>
<li><code>curl https://public.example.com/wrong -v</code> shows the default backend</li>
</ul>
</li>
<li>Detects <code>gamma</code>:
<ul>
<li>Adds nginx config to route to the services
<ul>
<li>Again, can be checked with: <code>kubectl exec -it &lt;ingress-controller-pod-name&gt; -- cat /etc/nginx/nginx.conf</code></li>
<li>This config will show the ssl_passthrough is set to not handle any TLS</li>
</ul>
</li>
<li>Updates the <code>Ingress</code> resource <code>Address</code> field with the DNS of the Cloud load balancer that the controller knows is pointing at it.</li>
<li><code>curl https://gamma.public.example.com -v</code> shows traffic routing correctly and with our in-application TLS handling</li>
</ul>
</li>
</ul>
</li>
<li><strong>external-dns:</strong>
<ul>
<li>Constantly checking for new/updated <code>Ingress</code> resources</li>
<li>Sees the two new resources but with empty <code>Address</code> fields - needs these to be set before it can point the specified host to the address the service sits behind.</li>
<li><code>Address</code> field is now updated for <code>alpha-beta</code>:
<ul>
<li>Checks if the hostname has already been created
<ul>
<li>external-dns defaults to using <code>txt</code> records in the chosen DNS service to persist information about the domains it manages</li>
</ul>
</li>
<li>Creates an <code>A</code> record pointing <code>public.example.com</code> to the value of <code>Address</code> in the ingress</li>
</ul>
</li>
<li><code>Address</code> field is now updated for <code>gamma</code>
<ul>
<li>This does the same as <code>alpha-beta</code> but for <code>gamma.public.example.com</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="-other-controllers-are-available">ü¶¶ Other controllers are available</h2>
<p>Other ingress controllers are available too, notable/interesting ones include:</p>
<ul>
<li><a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller#readme">AWS Load balancer controller</a> can manage ALBs out of cluster for ingress resources and NLBs for service resources</li>
<li><a href="https://doc.traefik.io/traefik/providers/kubernetes-ingress/">Traefix ingress controller</a> an ingress controller using the Kubernetes native edge router</li>
<li><a href="https://www.getambassador.io/docs/edge-stack/latest/topics/running/ingress-controller/">Ambassador</a> uses Envoy to act as an ingress controller and API gateway</li>
</ul>
<p>The benefit of having all of your configuration directly in Kubernetes as an <code>Ingress</code> resource comes from every compliant ingress-controllers reading and interacting with Ingress in the same way. The only change between them would be the implementation specific things you control via annotations, the general routes and hosts work regardless of your choice.</p>]]></description>
      
    </item>
    
    
    
    <item>
      <title>Kubernetes Networking #2: Services</title>
      <link>https://dgood.win/posts/kubernetes-networking-services/</link>
      <pubDate>Thu, 21 Apr 2022 10:06:09 +0000</pubDate>
      
      <guid>https://dgood.win/posts/kubernetes-networking-services/</guid>
      <description><![CDATA[<p>Kubernetes services are an essential resource everyone uses, but what are all the different types and how do they work under the hood?</p>
<p>In Kubernetes pods are ephemeral (or <a href="https://thenewstack.io/how-to-treat-your-kubernetes-clusters-like-cattle-not-pets/">cattle</a>), you should expect them to be killed, scaled and replaced whenever. So how do you talk to a set of pods which might have changed the next second? A service.</p>
<p>Services sit in front of your application and mean you only have to call the service address and magic Kubernetes components will get that request to the right Pod(s).
How does a service know which pods it can talk to?</p>
<h2 id="selectors">Selectors</h2>
<p>Services map to pods via label selectors. You have a pod with label <code>app=etizer</code> and then create a service with <code>.spec.selector</code> for the same and that service will forward traffic to any Ready pods matching that label.</p>
<p>Behind the scenes, a service resource also creates an <code>Endpoints</code> resource which maintains a list of Pod IPs based on your service selector. It‚Äôs worth noting here, the readyiness checks you implement for your Pods are what controls whether the IP goes into this <code>Endpoints</code> resource and allow traffic.</p>
<p>The exception here is selector-less services, these don‚Äôt have a label selector so won‚Äôt also have an <code>Endpoint</code> resource. To use these you also need to create an endpoint resource of the same name as your service, you can add any IPs and ports you want to this resource. This is a great way to map a service to something not in your cluster so your pods can talk to it as if it was a service.</p>
<p>But what are the different types of services and how do they work?</p>
<h2 id="service-types">Service Types</h2>
<h3 id="clusterip">ClusterIP</h3>
<p>When you create a <code>ClusterIP</code> type service you‚Äôll will get:</p>
<ul>
<li>A virtual IP for the service
<ul>
<li>This is within the <code>--service-cluster-ip-range</code> your <code>api-server</code> has been configured with</li>
<li><code>kube-proxy</code> is responsible for this virtual IP routing to one of your Pod IPs</li>
<li>This virtual IP will forward requests to one of your pod IPs based on the <code>Endpoints</code> resource associated with your service.</li>
</ul>
</li>
<li>A local cluster DNS address, eg. <code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code>
<ul>
<li>This is resolved by your <code>kube-dns</code> (CoreDNS) to your virtual service IP</li>
<li>This is why your service name must be <a href="https://datatracker.ietf.org/doc/html/rfc1035">RFC 1035 compliant</a></li>
</ul>
</li>
</ul>
<p>The normal way of utilising this would be giving your application the service DNS address so it can talk to another application running on the cluster. This DNS address resolves to the virtual IP and then the configuration <code>kube-proxy</code> controls handles that IP resolving to actual pod IP addresses. From there your CNI will route your request using the pod IP.</p>
<p>The <code>ClusterIP</code> service type is ‚ú®<em>special</em>‚ú®, whilst you use it as a way to expose your service internally within the cluster, it‚Äôs also used by types <code>NodePort</code> and <code>LoadBalancer</code> (by extension of LoadBalancer using <code>NodePort</code>) as part of their implementations.</p>
<figure><img src="/img/k8s-services-2-types.png"
    alt="Just like a Matryoshka doll"><figcaption>
      <h4>Just like a Matryoshka doll</h4><p>Just like a Matryoshka doll</p>
    </figcaption>
</figure>

<p><code>&lt;service-ip&gt;:&lt;service-port&gt; ‚Üí &lt;pod-ip&gt;:&lt;pod-port&gt;</code></p>
<h3 id="nodeport">NodePort</h3>
<p>A <code>NodePort</code> type service opens up a port on <strong>every node</strong> in your cluster, this port then proxies requests to your service‚Äôs virtual IP address.</p>
<p>The port range defaults to <code>30000-32767</code>, interestingly enough just below the Linux kernel default <a href="https://www.kernel.org/doc/html/latest//networking/ip-sysctl.html#ip-variables">ephemeral port range</a>. It can also be configured with the flag <code>--service-node-port-range</code> if you need these ports for some use case.</p>
<p>At a high level a <code>NodePort</code> is a single rule configured by <code>kube-proxy</code> onto every node, to forward requests to an underlying <code>ClusterIP</code> service.</p>
<p><code>&lt;node-ip&gt;:&lt;node-port&gt; ‚Üí &lt;service-ip&gt;:&lt;service-port&gt; ‚Üí &lt;pod-ip&gt;:&lt;pod-port&gt;</code></p>
<h3 id="loadbalancer">LoadBalancer</h3>
<p>A <code>LoadBalancer</code> type service only really works in a cloud environment. When created, asyncronously, the <a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">cloud-controller-manager</a> will provision you a load balancer which directs traffic to your pods.</p>
<p>The implementation of the different cloud load balancers varys wildly. It used to be different clouds were implemented <a href="https://github.com/kubernetes/kubernetes/tree/release-1.11/pkg/cloudprovider/providers/aws">in-tree</a>, committing directly to kubernetes but now it must be done via out-of-tree implementations, eg. <a href="https://github.com/kubernetes/cloud-provider-aws">cloud-provider-aws</a>.</p>
<p>Cloud implementations aside, once you have a <em>cloud</em> load balancer it works by pointing to a <code>NodePort</code> service running. This means that you have a load balancer with every Kubernetes worker node as a listener, forwarding traffic to any nodes on a particular port. From there it follows the standard <code>NodePort</code> ‚Üí <code>ClusterIP</code> route.</p>
<p>If you didn‚Äôt want to write a whole <a href="https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/">cloud control manager</a> to expose your application on your own servers, a <code>NodePort</code> service with that port pointed to from your existing load balancer would do the trick.</p>
<p><code>&lt;load-balancer-ip&gt;:&lt;lb-port&gt; -&gt; &lt;node-ip&gt;:&lt;node-port&gt; ‚Üí &lt;service-ip&gt;:service-port&gt; ‚Üí &lt;pod-ip&gt;:&lt;pod-port&gt;</code></p>
<h3 id="externalname">ExternalName</h3>
<p><code>ExternalName</code> services are the odd one out. Instead of using good ol‚Äô <code>ClusterIP</code> and getting a virtual IP, they create a <code>CNAME</code> record to map your service DNS to another DNS address.</p>
<p>As an example use case, lets say you have different environment databases, instead of service-owners changing the DNS depending on where they deploy their pods to you create a <code>ExternalName</code> service:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: Service
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: database
</span></span><span style="display:flex;"><span>  namespace: prod
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  type: ExternalName
</span></span><span style="display:flex;"><span>  externalName: prod.database.example.com
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: Service
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: database
</span></span><span style="display:flex;"><span>  namespace: dev
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  type: ExternalName
</span></span><span style="display:flex;"><span>  externalName: dev.database.example.com
</span></span></code></pre></div><p>Now all the service-owners have to do is point their code to <code>database.svc.cluster.local</code> and they will talk to the right database every time.</p>
<p><code>database.svc.cluster.local</code> = <code>CNAME test.database.example.com</code></p>
<p>It‚Äôs worth noting since the pod requesting the service will get the <code>CNAME</code> record back, the pod also has to be configured to resolve whatever domain the <code>CNAME</code> returns. If it‚Äôs a private record, you might want to <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/">configure revolvers in CoreDNS</a>.</p>
<h2 id="how-virtual-ips-work">How Virtual IPs work</h2>
<p><em>Most</em> services (sorry, <code>ExternalName</code>) use a virtual IP to forward requests to the proper pods. It‚Äôs the responsibility of <code>kube-proxy</code> to provide this virtual IP and make sure the traffic is actually forwarded. <code>kube-proxy</code> runs on every node so the steps below happen per worker node.</p>
<p>How this works depends on the mode of <code>kube-proxy</code> is given with <code>-proxy-mode</code> flag:</p>
<blockquote>
<p>‚ÑπÔ∏è <strong>For all of these <code>kube-proxy</code> will monitor the control plane for new/updated/removed <code>Service</code> and <code>Endpoint</code> resources and run the following steps</strong></p>
</blockquote>
<ul>
<li><strong>userspace</strong> (legacy)
<ul>
<li>For each <code>Service</code> it opens a random proxy port on the node</li>
<li>This proxy port is then told to iptables to capture traffic to this particular virtual IP and port to redirect to the Pod IPs in the <code>Endpoint</code> resource</li>
<li>The backend pod is chosen based on the <code>SessionAffinity</code> of the service</li>
<li><strong>The traffic here is actually proxied via <code>kube-proxy</code> itself</strong></li>
</ul>
</li>
<li><strong>iptables</strong>
<ul>
<li>For each <code>Service</code> it creates iptables rules which redirect traffic to the virtual IP and service port</li>
<li>These redirect rules point to sets of pod IPs based on what is in the endpoint resource</li>
<li>The backend pod is chosen at random by default</li>
<li><strong>The traffic is all handled within the Linux <a href="https://www.netfilter.org/">netfilter</a> so all within the kernelspace, <code>kube-proxy</code> doesn‚Äôt handle the traffic it just sets up rules to process it</strong></li>
</ul>
</li>
<li><strong>IPVS</strong>
<ul>
<li>Whilst iptables was designed to be a firewall and <code>kube-proxy</code> just uses it to redirect network traffic, IPVS was made for load balancing</li>
<li>At a high level this works similar to iptables, cluster IP and port ‚Üí set of pod IPs</li>
<li>IPVS is important when working at scale, if you have &gt;1000 services (or 10,000 pods) in your cluster IPVS is the performant choice</li>
</ul>
</li>
<li><strong>kernelspace</strong> (windows)</li>
</ul>
<h2 id="bringing-it-all-together">Bringing it all together</h2>
<p>Let‚Äôs create an example <code>LoadBalancer</code> service and see what components come together to get traffic from a client to your pods. We‚Äôll show how traffic from outside, via the load balancer, gets to your pods and how another pod in the cluster can also use this service without going via the load balancer.</p>
<p>In this example let‚Äôs say we‚Äôre running on a Kubernetes cluster in AWS with the AWS cloud controller manager properly configured, we have pods running with label <code>app=alpha</code> which listens on port <code>8080</code> for traffic and <code>kube-proxy</code> is in the default <code>iptables</code> mode.</p>
<ul>
<li>
<p>Service YAML</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: Service
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This doesn&#39;t affect routing but it&#39;s best practice to label components together</span>
</span></span><span style="display:flex;"><span>  labels:
</span></span><span style="display:flex;"><span>    app: alpha
</span></span><span style="display:flex;"><span>  name: example
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  ports:
</span></span><span style="display:flex;"><span>  - name: main
</span></span><span style="display:flex;"><span>    port: <span style="color:#ae81ff">8081</span>
</span></span><span style="display:flex;"><span>    protocol: TCP
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Target port is the port your pod has open</span>
</span></span><span style="display:flex;"><span>    targetPort: <span style="color:#ae81ff">8080</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This is what tells the service which pods to route to</span>
</span></span><span style="display:flex;"><span>  selector:
</span></span><span style="display:flex;"><span>    app: alpha
</span></span><span style="display:flex;"><span>  type: LoadBalancer
</span></span></code></pre></div></li>
</ul>
<p>Lets create the service and have a look:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ kubectl apply -f service.yaml
</span></span><span style="display:flex;"><span>service/example created
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl get service example
</span></span><span style="display:flex;"><span>NAME      TYPE           CLUSTER-IP      EXTERNAL-IP     PORT<span style="color:#f92672">(</span>S<span style="color:#f92672">)</span>          AGE
</span></span><span style="display:flex;"><span>example   LoadBalancer   10.43.205.153   192.168.0.147   8081:32255/TCP   4s
</span></span></code></pre></div><p>I now have:</p>
<ul>
<li>An Endpoints resource corresponding to ready pods matching my label selector
<ul>
<li><code>kubectl get endpoints example</code></li>
</ul>
</li>
<li>A cluster IP setup for my service <code>10.43.205.153</code></li>
<li>A DNS address for the service configured
<ul>
<li>(from inside a Pod)</li>
<li><code>dig example.default.svc.cluster.local</code></li>
</ul>
</li>
<li>A external (to the cluster) IP which maps to my load balancer <code>192.168.0.147</code></li>
<li>A NodePort setup <code>32255</code> so all my workers now have this port open
<ul>
<li><code>netstat -tulnp | grep 32255</code></li>
</ul>
</li>
<li>iptables rules setup
<ul>
<li><code>sudo iptables -L | grep example</code></li>
<li><code>sudo iptables -L | grep 10.43.205.153</code></li>
</ul>
</li>
</ul>
<p>Lets see how this request goes through to our pod from outside the cluster and a pod within.</p>
<h3 id="from-outside-the-cluster">From outside the cluster</h3>

  <figure class="left" >
    <a href="/img/k8s-services-2-lb-external.png" >
    <img src="/img/k8s-services-2-lb-external.png"   />
    
    </a>
  </figure>


<ol>
<li>The client makes a request to the load balancer to try and talk to the application, for how this is configured see my post on ingress into a cluster</li>
<li>The load balancer selects one of the configured listeners, this is usually either random or round robin but depends on your load balancer and configuration</li>
<li>The node has port 32255 open, requests here are configured in iptables to forward the request to the virtual IP of the service. These rules were configured by <code>kube-proxy</code></li>
<li>iptables then resolves the virtual/service IP into a Pod IP address, one is chosen by random and the request heads there. Now the traffic is inside the cluster the CNI controls the network movements between nodes (see <a href="https://dgood.win/post/kubernetes-networking-overview/">here</a> for more info on what the CNI sets up)</li>
<li>Once the request has reached the node with the pod on it, the CNI and iptables will forward it to the Pod as expected</li>
</ol>
<h3 id="from-a-pod-within-the-cluster">From a pod within the cluster</h3>

  <figure class="left" >
    <a href="/img/k8s-services-2-lb-internal.png" >
    <img src="/img/k8s-services-2-lb-internal.png"   />
    
    </a>
  </figure>


<ol>
<li>The requesting pod will have been configured with the service DNS address <code>example.default.svc.cluster.local</code>. As it makes this request from within the cluster CoreDNS running will be the DNS resolver. This request will be resolved to the Service (virtual) IP address: <code>10.43.205.153</code></li>
<li>Making a request to this address will hit the rules in iptables, configured by <code>kube-proxy</code>, this resolves the service IP into a Pod IP by randomly selecting a pod</li>
<li>The rest of the request continues as in the external example.</li>
</ol>]]></description>
      
    </item>
    
    
    
    <item>
      <title>Kubernetes Networking #1</title>
      <link>https://dgood.win/posts/kubernetes-networking-overview/</link>
      <pubDate>Mon, 10 Jan 2022 12:06:09 +0000</pubDate>
      
      <guid>https://dgood.win/posts/kubernetes-networking-overview/</guid>
      <description><![CDATA[<p>Kubernetes networking can be a bit of a rabbit hole to debug and discover what your pods are actually doing. In this post we‚Äôll explore at a high level how a request goes from 1 pod to another and which components control each.</p>
<h2 id="components">Components</h2>
<p>The components we‚Äôre going to look at today are:</p>
<ul>
<li><code>kube-dns/coredns</code> resolves domain names for services (and pods) into an IP address
<ul>
<li>‚ÑπÔ∏è <code>kube-dns</code> was replaced by <code>coredns</code> but the in cluster service is still called <code>kube-dns</code></li>
<li>This is the resolver for any DNS requests inside your pods, if you wanted to resolve internal service domains or to a private DNS resolver, you could configure this to forward those requests.</li>
</ul>
</li>
<li><code>kube-proxy</code> resolves network requests for the IP address of a service to the IP addresses of pods which match that service. This has different modes:
<ul>
<li><a href="https://en.wikipedia.org/wiki/Iptables">iptables</a> (default) - To see your iptables rules: <code>iptables -L</code></li>
<li><a href="https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/">ipvs</a> - To see your ipvs rules <code>ipvsadm -Ln</code></li>
</ul>
</li>
<li>Your chosen <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#calico">CNI</a>, there‚Äôs a range of these, all which work differently. Generally a CNI will configure any networking around a Pod IP, routing to and from pods to:
<ul>
<li>Ensure pod network requests are routed properly to other pods</li>
<li>Ensure pod network requests can reach outside the cluster</li>
</ul>
</li>
</ul>
<p>An easy way of thinking about it can be, resolving between a service and healthy pods is <code>kube-dns</code> and <code>kube-proxy</code> but then that request will need to go through routing setup by the <code>CNI</code>. For cloud environments, your <code>CNI</code> will usually handle (or provide ways to setup) things like cross-subnet traffic as well.</p>
<p>To follow the resolution between components, say we wanted to go to a service called <code>myapp</code> which sits in namespace <code>namespace001</code> which had 2 healthy pods matching the service selector. The resolution would be:</p>
<ol>
<li><strong>CoreDNS:</strong> (service FQDN)<code>myapp.namespace001.svc.cluster.local</code> ‚Üí <code>172.16.10.10</code> (service cluster IP)</li>
<li><strong>iptables:</strong> <code>172.16.10.10</code> ‚Üí <code>10.10.10.10</code> (a pod IP, selected at random from healthy pods)</li>
<li><strong>CNI:</strong> Will route traffic for <code>10.10.10.10</code> from the requester node, to the target node</li>
</ol>
<h2 id="example-journey-of-a-request">Example journey of a request</h2>
<p>As an example, let‚Äôs explore in an example Kubernetes cluster how a pod will send a network request to another pod.
For this example:</p>
<ul>
<li>The pod will try to talk to a pod on another node then itself</li>
<li>The 2 nodes will be in seperate subnets
<ul>
<li>Routing between subnets is presumed to be setup and fine and we‚Äôll abstract that to a <code>router</code> in the diagram</li>
</ul>
</li>
<li>The <code>CNI</code> will be Calico, but we won‚Äôt be looking too much into its internals</li>
<li><strong>pod-0</strong> will send a request to <strong>pod-3</strong> through a service</li>
<li>We‚Äôll assume Kubernetes and Calico are setup with defaults:
<ul>
<li>iptables mode for <code>kube-proxy</code></li>
<li>vxlan encapsulation for cross subnet traffic</li>
</ul>
</li>
</ul>

  <figure class="left" >
    <a href="/img/k8s_network_request_example.png" >
    <img src="/img/k8s_network_request_example.png"   />
    
    </a>
  </figure>


<ul>
<li>
<p><strong>1</strong> <strong>pod-0</strong> requests the service‚Äôs Cluster IP from <code>kube-dns</code></p>
<ul>
<li><code>kube-dns</code> has dynamic config which updates mapping the service name to a Service IP address lets say <code>172.16.10.10</code>
<ul>
<li>`kubectl exec command for coredns</li>
</ul>
</li>
<li>*technically this whole request to <code>kube-dns</code> goes via the same iptables‚Üírouting tables path since <code>kube-dns</code> runs in pods on the cluster. The only key difference is the DNS resolver is an IP address so it doesn‚Äôt ask <code>kube-dns</code> to define <code>kube-dns</code>. But trying to show aswell would be madness.</li>
</ul>
</li>
<li>
<p><strong>2</strong> With the cluster IP (<code>172.16.10.10</code>) the request hits iptables</p>
<ul>
<li>iptables selects from a random one of the pod IP addresses for this particular service IP
<ul>
<li><code>iptables -L</code> or <code>iptables-save</code></li>
</ul>
</li>
<li>These rules are all loaded in here and dynamically updated by `kube-proxy If a pod starts failing Readiness checks for instance, it gets removed from this list.</li>
</ul>
</li>
<li>
<p><strong>3</strong> The Pod IP is now resolved in the nodes routing tables <code>sudo route -n</code> or <code>ip route list</code></p>
<ul>
<li>These routing table rules will be configured based on the network environment the node sits in. Calico, running locally, will also configure rules for pod networking.</li>
<li>Based on the Pod IP, it‚Äôll be resolved through multiple sets of rules:
<ul>
<li>This node will have a range of Pod IPs which resolve to the local node itself</li>
<li>This node will also have an IP range for nodes inside the same subnet - these could avoid encapsulation for performance gain</li>
<li>This node also has range(s) for other subnets, if it‚Äôs not local to the node or subnet, it then gets routed to the subnet routing table. For these rules, Calico might instead route traffic to itself to be encapsulated before crossing the network boundary.</li>
<li>There also might be a range for external traffic not local to the network, but if this was a private subnet this is most likely behind a NAT gateway inside the local network</li>
</ul>
</li>
<li>This particular request is destined for another subnet. The routing table has directed the request to the Calico pod running locally on this node to handle routing this.</li>
</ul>
</li>
<li>
<p><strong>4</strong> Calico will now encapsulate the network request to allow it to pass subnet boundaries <code>calicoctl node status</code></p>
<ul>
<li>By encapsulating the request‚Äôs original networking information is wrapped up as data and new network info is added</li>
<li>Calico will send this as a vxlan or ipinip packet to the required node in another subnet</li>
<li>The encapsulated packet (shown as an orange line) will:
<ul>
<li>Go back through the route table on the node itself</li>
<li>To the local subnet router, which will resolve which subnet it needs to go into</li>
<li>Hit the destination subnet routing table which can direct it to the node</li>
<li>To the other node itself</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>5</strong> Once the encapsulted packet arrived at the other node, it‚Äôll go through iptables rules to see if it‚Äôs allowed traffic.</p>
<ul>
<li>This is why you might need to add the encapsulated packets into your firewall rules to be allowed in.</li>
</ul>
</li>
<li>
<p><strong>6</strong> The routing table on this node will now:</p>
<ul>
<li>Send the encapsulated packet to the local Calico to be unencapsulated</li>
<li>The original (unencapsulated) network request will resolve again in iptables and routing tables rules to determine where it should go
<ul>
<li>This could also cause the packet to have to be routed again since the destination has moved</li>
</ul>
</li>
<li>The request is directed to the pod running locally</li>
</ul>
</li>
</ul>
<h3 id="there-are-a-lot-of-things-missed-off-here">There are a lot of things missed off here:</h3>
<ul>
<li><a href="http://www.adminsehow.com/2011/09/iptables-packet-traverse-map/">Technically iptables is interacted with before, after (and sometimes during) a routing decision</a></li>
<li>How the pod sits within it‚Äôs own <a href="https://blog.scottlowe.org/2013/09/04/introducing-linux-network-namespaces/">network namespace</a> with a virtual ethernet interface for traffic coming to the pod</li>
<li>How Calico uses <a href="https://projectcalico.docs.tigera.io/reference/felix/">Felix</a> to program it‚Äôs routes and ACLs and <a href="https://bird.network.cz/">BIRD</a> for distributing the routes to BGP peers on the network</li>
<li><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">Alternate CNIs</a></li>
<li>How the whole kube-proxy aspects could be stipped out for an eBPF approach (Calico‚Äôs <a href="https://projectcalico.docs.tigera.io/maintenance/ebpf/enabling-bpf">eBPF dataplane</a> or Cilium and it‚Äôs <a href="https://docs.cilium.io/en/v1.9/gettingstarted/kubeproxy-free/">kube proxy replacement</a>)</li>
</ul>
<h2 id="related-docs">Related docs:</h2>
<ul>
<li><a href="https://projectcalico.docs.tigera.io/reference/architecture/overview">Calico Component Architecture</a></li>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy reference</a></li>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">Kubernetes DNS docs</a></li>
</ul>]]></description>
      
    </item>
    
    
    
    <item>
      <title>Kubernetes Debugging</title>
      <link>https://dgood.win/posts/kubernetes-debugging/</link>
      <pubDate>Mon, 20 Apr 2020 14:57:09 +0100</pubDate>
      
      <guid>https://dgood.win/posts/kubernetes-debugging/</guid>
      <description><![CDATA[<p>This is a workflow to debug common Kubernetes problems when deploying applications.</p>
<p>Click on the image to see it in further detail</p>

  <figure class="left" >
    <a href="/img/k8s_debugging_v1.svg" >
    <img src="/img/k8s_debugging_v1.svg"   />
    
    </a>
  </figure>]]></description>
      
    </item>
    
    
  </channel>
</rss>
